name: Deploy to EKS

on:
  push:
    branches:
      - master

permissions:
  contents: read

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install AWS CLI
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install --update
          aws --version
        continue-on-error: false

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: '1.5.0'

      - name: Terraform Init
        run: |
          terraform init
        working-directory: ./terraform
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Import EKS Node Group
        run: |
          if ! terraform state show aws_eks_node_group.django_microservice >/dev/null 2>&1; then
            echo "Importing existing EKS node group..."
            if terraform import aws_eks_node_group.django_microservice django-microservice-cluster:django-microservice-nodes; then
              echo "Import successful"
              terraform state list
            else
              echo "Import failed. Listing available node groups..."
              aws eks list-nodegroups --cluster-name django-microservice-cluster --region ${{ secrets.AWS_REGION }}
              exit 1
            fi
          else
            echo "EKS node group already in Terraform state"
            terraform state list
          fi
        working-directory: ./terraform
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Import EBS CSI Driver IAM Role
        run: |
          ROLE_NAME="django-microservice-ebs-csi-driver-role"
          if ! terraform state show aws_iam_role.ebs_csi_driver >/dev/null 2>&1; then
            echo "IAM role $ROLE_NAME not in Terraform state."
            if aws iam get-role --role-name $ROLE_NAME --region ${{ secrets.AWS_REGION }} >/dev/null 2>&1; then
              echo "Attempting to import IAM role $ROLE_NAME..."
              terraform import aws_iam_role.ebs_csi_driver $ROLE_NAME || echo "Import failed."
            else
              echo "IAM role $ROLE_NAME does not exist in AWS."
            fi
          else
            echo "IAM role $ROLE_NAME already in Terraform state."
          fi
        working-directory: ./terraform
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Import Kubernetes Service for Postgres
        run: |
          SERVICE_NAME="db"
          NAMESPACE="default"
          RESOURCE_ADDRESS="kubernetes_service.postgres_service"
          aws eks update-kubeconfig --name django-microservice-cluster --region ${{ secrets.AWS_REGION }}
          if ! terraform state show $RESOURCE_ADDRESS >/dev/null 2>&1; then
            echo "Kubernetes service $SERVICE_NAME not in Terraform state."
            if kubectl get service $SERVICE_NAME -n $NAMESPACE >/dev/null 2>&1; then
              echo "Attempting to import Kubernetes service $SERVICE_NAME..."
              terraform import $RESOURCE_ADDRESS $NAMESPACE/$SERVICE_NAME || echo "Import failed."
            else
              echo "Kubernetes service $SERVICE_NAME does not exist in cluster."
            fi
          else
            echo "Kubernetes service $SERVICE_NAME already in Terraform state."
          fi
        working-directory: ./terraform
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Terraform Plan
        run: |
          terraform plan -var="region=${{ secrets.AWS_REGION }}" -out=tfplan -json > terraform-plan.log 2>&1 || (cat terraform-plan.log && exit 1)
          cat terraform-plan.log
        working-directory: ./terraform
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Terraform Apply
        run: |
          terraform apply -auto-approve -json tfplan > terraform-apply.log 2>&1 || (cat terraform-apply.log && exit 1)
          cat terraform-apply.log
        working-directory: ./terraform
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      - name: Extract Terraform Outputs
        id: terraform_outputs
        run: |
          echo "ECR_REPO_URL=$(terraform output -raw ecr_repository_url)" >> $GITHUB_ENV
        working-directory: ./terraform

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Update Kubeconfig
        run: |
          aws --version
          echo "Listing EKS clusters in region ${{ secrets.AWS_REGION }}..."
          aws eks list-clusters --region ${{ secrets.AWS_REGION }}
          echo "Updating kubeconfig for cluster django-microservice-cluster..."
          aws eks update-kubeconfig --name django-microservice-cluster --region ${{ secrets.AWS_REGION }}
          kubectl config current-context
          kubectl cluster-info
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Check for microservices-k8.yml
        run: |
          if [ ! -f "microservices-k8.yml" ]; then
            echo "Error: microservices-k8.yml not found"
            exit 1
          fi

      - name: Substitute ECR_REPO_URL
        run: |
          sed -i "s|\${ECR_REPO_URL}:latest|${{ env.ECR_REPO_URL }}:${{ env.IMAGE_TAG }}|g" microservices-k8.yml

      - name: Create Kubernetes Secret
        run: |
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Secret
          metadata:
            name: app-secrets
            namespace: default
          type: Opaque
          stringData:
            DB_NAME: "${{ secrets.DB_NAME }}"
            DB_USER: "${{ secrets.DB_USER }}"
            DB_PASSWORD: "${{ secrets.DB_PASSWORD }}"
            DB_HOST: "db"
            SECRET_KEY: "${{ secrets.SECRET_KEY }}"
          EOF

      - name: Deploy to EKS
        run: |
          kubectl apply -f microservices-k8.yml --validate=true
          kubectl rollout status deployment/postgres -n default --timeout=10m
          kubectl rollout status deployment/redis -n default --timeout=10m
          kubectl wait --for=condition=complete job/django-migrate -n default --timeout=5m
          kubectl rollout status deployment/web -n default --timeout=10m
          kubectl rollout status deployment/worker -n default --timeout=10m

      - name: Verify deployment
        run: |
          kubectl get pods -n default -o wide
          kubectl get svc -n default
          kubectl get pvc -n default

      - name: Debug on failure
        if: failure()
        run: |
          kubectl get pods -n kube-system || echo "kubectl failed"
          kubectl get pods -n default || echo "kubectl failed"
          kubectl describe pod -l app=postgres -n default || echo "kubectl failed"
          kubectl describe pod -l app=django-app,component=web -n default || echo "kubectl failed"
          kubectl logs -l app=django-app,component=web -n default || echo "kubectl failed"
          kubectl describe pvc postgres-pvc -n default || echo "kubectl failed"
          kubectl get events -n default || echo "kubectl failed"
          kubectl get configmap -n kube-system -l "owner=helm" || echo "kubectl failed"
          cat terraform/terraform-plan.log || echo "Terraform plan log not found"
          cat terraform/terraform-apply.log || echo "Terraform apply log not found"
          aws eks describe-cluster --name django-microservice-cluster --region ${{ secrets.AWS_REGION }} || echo "AWS CLI failed"

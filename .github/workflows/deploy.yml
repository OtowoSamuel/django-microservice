name: Build and Deploy

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:  # Allows manual triggering

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: django-microservice
  EKS_CLUSTER_NAME: django-microservice-cluster  # Updated to match your actual cluster name

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v1
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1

    - name: Build, tag, and push image to Amazon ECR
      id: build-image
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        # Build a docker container and push it to ECR
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        
        # Also tag as latest
        docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:latest
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
        
        echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT
    
    - name: Install kubectl
      uses: azure/setup-kubectl@v3
      
    - name: Update kubeconfig
      run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
    
    - name: Process Kubernetes manifests
      run: |
        export ECR_REPO_URL=${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}
        
        # Process the manifest file to replace variables
        cat microservices-k8.yml | \
        sed "s|\${ECR_REPO_URL}|$ECR_REPO_URL|g" > microservices-k8-processed.yml
        
    - name: Deploy core infrastructure
      run: |
        # Apply everything except the migration job first
        grep -v "name: django-migrate" microservices-k8-processed.yml > core-infrastructure.yml
        kubectl apply -f core-infrastructure.yml
        
        # Wait for database to be ready
        echo "Waiting for database to be ready..."
        kubectl wait --for=condition=available --timeout=300s deployment/postgres
        sleep 30  # Give the database a bit more time to initialize
    
    - name: Run database migrations
      run: |
        # Generate a unique job name with commit hash
        COMMIT_HASH=$(echo ${{ github.sha }} | cut -c1-40)
        JOB_NAME="django-migrate-$COMMIT_HASH"
        
        # Extract the migration job and add a unique name
        cat microservices-k8-processed.yml | \
          sed -n '/# Django Migrate Job/,/---/p' | \
          sed "s/name: django-migrate/name: $JOB_NAME/" > migration-job.yml
        
        echo "Creating migration job..."
        kubectl apply -f migration-job.yml
        
        echo "Waiting for migrations to complete..."
        
        # Wait with increased timeout and better error handling
        if ! kubectl wait --for=condition=complete --timeout=180s job/$JOB_NAME; then
          echo "Migration job didn't complete in time, checking logs..."
          # If wait fails, get the pod name and check logs
          POD_NAME=$(kubectl get pods -l job-name=$JOB_NAME -o jsonpath='{.items[0].metadata.name}')
          echo "Migration logs:"
          kubectl logs $POD_NAME --all-containers || true
          kubectl describe pod $POD_NAME || true
          
          # Check if job is still running
          if kubectl get job $JOB_NAME -o jsonpath='{.status.active}' | grep -q 1; then
            echo "Job is still running, giving it more time..."
            sleep 60
            kubectl wait --for=condition=complete --timeout=60s job/$JOB_NAME || true
            kubectl logs $(kubectl get pods -l job-name=$JOB_NAME -o jsonpath='{.items[0].metadata.name}') --all-containers || true
          fi
          
          # Final check
          if kubectl get job $JOB_NAME -o jsonpath='{.status.succeeded}' | grep -q 1; then
            echo "Migration succeeded after additional wait"
          else
            echo "Migration failed"
            exit 1
          fi
        fi
    
    - name: Restart deployments
      run: |
        kubectl rollout restart deployment web
        kubectl rollout restart deployment worker
        kubectl rollout status deployment web --timeout=180s
        kubectl rollout status deployment worker --timeout=180s
    
    - name: Get service URL
      run: |
        echo "Application deployed successfully!"
        LOAD_BALANCER=$(kubectl get svc -o jsonpath='{.items[?(@.spec.type=="LoadBalancer")].status.loadBalancer.ingress[0].hostname}')
        if [ -n "$LOAD_BALANCER" ]; then
          echo "Application is available at: http://$LOAD_BALANCER/"
        else
          echo "No LoadBalancer found."
        fi